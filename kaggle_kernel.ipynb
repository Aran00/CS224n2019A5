{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"id":"EWkcSzCL-SUF","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"file_path = '/kaggle/input/'\n\nargs = {\n    \"--cuda\": True,\n    \"--train-src\": file_path + \"train.es\",\n    \"--train-tgt\": file_path + \"train.en\",\n    \"--dev-src\": file_path + \"dev.es\",\n    \"--dev-tgt\": file_path + \"dev.en\",\n    \"--vocab\": file_path + \"vocab.json\",\n    \"--seed\": 0,\n    \"--batch-size\": 32,\n    \"--embed-size\": 256,\n    \"--hidden-size\": 256,\n    \"--clip-grad\": 5.0,\n    \"--log-every\": 10,\n    \"--max-epoch\": 10,\n    \"--input-feed\": True,\n    \"--patience\": 5,\n    \"--max-num-trial\": 5,\n    \"--lr-decay\": 0.5,\n    \"--beam-size\": 5,\n    \"--sample-size\": 5,\n    \"--lr\": 0.001,\n    \"--uniform-init\": 0.1,\n    \"--save-to\": \"model.bin\",\n    \"--valid-niter\": 2000,\n    \"--dropout\": 0.3,\n    \"--max-decoding-time-step\": 70,\n    \"--no-char-decoder\": False,\n    \"train\": True\n}","execution_count":null,"outputs":[]},{"metadata":{"id":"HahpUeaL9-gH","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n\"\"\"\nCS224N 2018-19: Homework 5\n\"\"\"\n\nfrom collections import namedtuple, Counter\nimport sys\nimport pickle\nimport time\nfrom typing import List, Tuple, Dict, Set, Union\nimport torch\nimport torch.nn as nn\nimport torch.nn.utils\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\nimport random\nfrom functools import reduce\nimport numpy as np\nimport math\nfrom itertools import chain\nimport json\nfrom nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n","execution_count":null,"outputs":[]},{"metadata":{"id":"5VgtXbcM9-gJ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def pad_sents_char(sents, char_pad_token):\n    \"\"\" Pad list of sentences according to the longest sentence in the batch and max_word_length.\n    @param sents (list[list[list[int]]]): list of sentences, result of `words2charindices()`\n        from `vocab.py`\n    @param char_pad_token (int): index of the character-padding token\n    @returns sents_padded (list[list[list[int]]]): list of sentences where sentences/words shorter\n        than the max length sentence/word are padded out with the appropriate pad token, such that\n        each sentence in the batch now has same number of words and each word has an equal\n        number of characters\n        Output shape: (batch_size, max_sentence_length, max_word_length)\n    \"\"\"\n    # Words longer than 21 characters should be truncated\n    max_word_length = 21\n\n    ### YOUR CODE HERE for part 1f\n    ### TODO:\n    ###     Perform necessary padding to the sentences in the batch similar to the pad_sents()\n    ###     method below using the padding character from the arguments. You should ensure all\n    ###     sentences have the same number of words and each word has the same number of\n    ###     characters.\n    ###     Set padding words to a `max_word_length` sized vector of padding characters.\n    ###\n    ###     You should NOT use the method `pad_sents()` below because of the way it handles\n    ###     padding and unknown words.\n    max_sent_length = max([len(sent) for sent in sents])\n    padding_word = max_word_length * [char_pad_token]\n    sents_padded = [[word + (max_word_length - len(word)) * [char_pad_token]\n                     if len(word) < max_word_length else word[:max_word_length] for word in sent]\n                    + (max_sent_length - len(sent)) * [padding_word] for sent in sents]\n    ### END YOUR CODE\n    return sents_padded\n\n\ndef pad_sents(sents, pad_token):\n    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n    @param sents (list[list[int]]): list of sentences, where each sentence\n                                    is represented as a list of words\n    @param pad_token (int): padding token\n    @returns sents_padded (list[list[int]]): list of sentences where sentences shorter\n        than the max length sentence are padded out with the pad_token, such that\n        each sentences in the batch now has equal length.\n        Output shape: (batch_size, max_sentence_length)\n    \"\"\"\n    sents_padded = []\n\n    ### COPY OVER YOUR CODE FROM ASSIGNMENT 4\n    sents_len = list(map(lambda x: len(x), sents))\n    max_len = reduce(lambda x, y: x if x > y else y, sents_len, 0)\n    sents_padded = [sent + [pad_token] * (max_len - len(sent)) for sent in sents]\n    ### END YOUR CODE FROM ASSIGNMENT 4\n\n    return sents_padded\n\n\n\ndef read_corpus(file_path, source):\n    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n    @param file_path (str): path to file containing corpus\n    @param source (str): \"tgt\" or \"src\" indicating whether text\n        is of the source language or target language\n    \"\"\"\n    data = []\n    for line in open(file_path):\n        sent = line.strip().split(' ')\n        # only append <s> and </s> to the target sentence\n        if source == 'tgt':\n            sent = ['<s>'] + sent + ['</s>']\n        data.append(sent)\n\n    return data\n\n\ndef batch_iter(data, batch_size, shuffle=False):\n    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n    @param batch_size (int): batch size\n    @param shuffle (boolean): whether to randomly shuffle the dataset\n    \"\"\"\n    batch_num = math.ceil(len(data) / batch_size)\n    index_array = list(range(len(data)))\n\n    if shuffle:\n        np.random.shuffle(index_array)\n\n    for i in range(batch_num):\n        indices = index_array[i * batch_size: (i + 1) * batch_size]\n        examples = [data[idx] for idx in indices]\n\n        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n        src_sents = [e[0] for e in examples]\n        tgt_sents = [e[1] for e in examples]\n\n        yield src_sents, tgt_sents\n","execution_count":null,"outputs":[]},{"metadata":{"id":"lK-Iq52s9-gM","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"class VocabEntry(object):\n    \"\"\" Vocabulary Entry, i.e. structure containing either\n    src or tgt language terms.\n    \"\"\"\n    def __init__(self, word2id=None):\n        \"\"\" Init VocabEntry Instance.\n        @param word2id (dict): dictionary mapping words 2 indices\n        \"\"\"\n        if word2id:\n            self.word2id = word2id\n        else:\n            self.word2id = dict()\n            self.word2id['<pad>'] = 0   # Pad Token\n            self.word2id['<s>'] = 1     # Start Token\n            self.word2id['</s>'] = 2    # End Token\n            self.word2id['<unk>'] = 3   # Unknown Token\n        self.unk_id = self.word2id['<unk>']\n        self.id2word = {v: k for k, v in self.word2id.items()}\n        \n        ## Additions to the A4 code:\n        self.char_list = list(\"\"\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]\"\"\")\n\n        self.char2id = dict() # Converts characters to integers\n        self.char2id['<pad>'] = 0\n        self.char2id['{'] = 1\n        self.char2id['}'] = 2\n        self.char2id['<unk>'] = 3\n        for i, c in enumerate(self.char_list):\n            self.char2id[c] = len(self.char2id)\n        self.char_unk = self.char2id['<unk>']\n        self.start_of_word = self.char2id[\"{\"]\n        self.end_of_word = self.char2id[\"}\"]\n        assert self.start_of_word+1 == self.end_of_word\n\n        self.id2char = {v: k for k, v in self.char2id.items()} # Converts integers to characters\n        ## End additions to the A4 code\n\n    def __getitem__(self, word):\n        \"\"\" Retrieve word's index. Return the index for the unk\n        token if the word is out of vocabulary.\n        @param word (str): word to look up.\n        @returns index (int): index of word \n        \"\"\"\n        return self.word2id.get(word, self.unk_id)\n\n    def __contains__(self, word):\n        \"\"\" Check if word is captured by VocabEntry.\n        @param word (str): word to look up\n        @returns contains (bool): whether word is contained    \n        \"\"\"\n        return word in self.word2id\n\n    def __setitem__(self, key, value):\n        \"\"\" Raise error, if one tries to edit the VocabEntry.\n        \"\"\"\n        raise ValueError('vocabulary is readonly')\n\n    def __len__(self):\n        \"\"\" Compute number of words in VocabEntry.\n        @returns len (int): number of words in VocabEntry\n        \"\"\"\n        return len(self.word2id)\n\n    def __repr__(self):\n        \"\"\" Representation of VocabEntry to be used\n        when printing the object.\n        \"\"\"\n        return 'Vocabulary[size=%d]' % len(self)\n\n    def id2word(self, wid):\n        \"\"\" Return mapping of index to word.\n        @param wid (int): word index\n        @returns word (str): word corresponding to index\n        \"\"\"\n        return self.id2word[wid]\n\n    def add(self, word):\n        \"\"\" Add word to VocabEntry, if it is previously unseen.\n        @param word (str): word to add to VocabEntry\n        @return index (int): index that the word has been assigned\n        \"\"\"\n        if word not in self:\n            wid = self.word2id[word] = len(self)\n            self.id2word[wid] = word\n            return wid\n        else:\n            return self[word]\n\n    def words2charindices(self, sents):\n        \"\"\" Convert list of sentences of words into list of list of list of character indices.\n        @param sents (list[list[str]]): sentence(s) in words\n        @return word_ids (list[list[list[int]]]): sentence(s) in indices\n        \"\"\"\n        ### YOUR CODE HERE for part 1e\n        ### TODO: \n        ###     This method should convert characters in the input sentences into their \n        ###     corresponding character indices using the character vocabulary char2id \n        ###     defined above.\n        ###\n        ###     You must prepend each word with the `start_of_word` character and append \n        ###     with the `end_of_word` character.\n        return [[[self.start_of_word] + [self.char2id[char] for char in word] + [self.end_of_word] for word in sent]\n                for sent in sents]\n        ### END YOUR CODE\n\n    def words2indices(self, sents):\n        \"\"\" Convert list of sentences of words into list of list of indices.\n        @param sents (list[list[str]]): sentence(s) in words\n        @return word_ids (list[list[int]]): sentence(s) in indices\n        \"\"\"\n        return [[self[w] for w in s] for s in sents]\n\n    def indices2words(self, word_ids):\n        \"\"\" Convert list of indices into words.\n        @param word_ids (list[int]): list of word ids\n        @return sents (list[str]): list of words\n        \"\"\"\n        return [self.id2word[w_id] for w_id in word_ids]\n\n    def to_input_tensor_char(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n        shorter sentences.\n\n        @param sents (List[List[str]]): list of sentences (words)\n        @param device: device on which to load the tensor, i.e. CPU or GPU\n\n        @returns sents_var: tensor of (max_sentence_length, batch_size, max_word_length)\n        \"\"\"\n        ### YOUR CODE HERE for part 1g\n        ### TODO: \n        ###     Connect `words2charindices()` and `pad_sents_char()` which you've defined in \n        ###     previous parts\n        sents_char_ids = self.words2charindices(sents)\n        padded_sents_char_ids = pad_sents_char(sents_char_ids, self.char2id['<pad>'])\n        sents_var = torch.tensor(padded_sents_char_ids, dtype=torch.long, device=device)\n        return sents_var.transpose(0, 1).contiguous()  #sents_var.permute(1, 0, 2)\n        ### END YOUR CODE\n\n    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n        shorter sentences.\n\n        @param sents (List[List[str]]): list of sentences (words)\n        @param device: device on which to load the tesnor, i.e. CPU or GPU\n\n        @returns sents_var: tensor of (max_sentence_length, batch_size)\n        \"\"\"\n        word_ids = self.words2indices(sents)\n        sents_t = pad_sents(word_ids, self['<pad>'])\n        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n        return torch.t(sents_var)\n\n    @staticmethod\n    def from_corpus(corpus, size, freq_cutoff=2):\n        \"\"\" Given a corpus construct a Vocab Entry.\n        @param corpus (list[str]): corpus of text produced by read_corpus function\n        @param size (int): # of words in vocabulary\n        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n        \"\"\"\n        vocab_entry = VocabEntry()\n        word_freq = Counter(chain(*corpus))\n        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n              .format(len(word_freq), freq_cutoff, len(valid_words)))\n        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n        for word in top_k_words:\n            vocab_entry.add(word)\n        return vocab_entry\n\n\nclass Vocab(object):\n    \"\"\" Vocab encapsulating src and target langauges.\n    \"\"\"\n    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n        \"\"\" Init Vocab.\n        @param src_vocab (VocabEntry): VocabEntry for source language\n        @param tgt_vocab (VocabEntry): VocabEntry for target language\n        \"\"\"\n        self.src = src_vocab\n        self.tgt = tgt_vocab\n\n    @staticmethod\n    def build(src_sents, tgt_sents, vocab_size, freq_cutoff) -> 'Vocab':\n        \"\"\" Build Vocabulary.\n        @param src_sents (list[str]): Source sentences provided by read_corpus() function\n        @param tgt_sents (list[str]): Target sentences provided by read_corpus() function\n        @param vocab_size (int): Size of vocabulary for both source and target languages\n        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word.\n        \"\"\"\n        assert len(src_sents) == len(tgt_sents)\n\n        print('initialize source vocabulary ..')\n        src = VocabEntry.from_corpus(src_sents, vocab_size, freq_cutoff)\n\n        print('initialize target vocabulary ..')\n        tgt = VocabEntry.from_corpus(tgt_sents, vocab_size, freq_cutoff)\n\n        return Vocab(src, tgt)\n\n    def save(self, file_path):\n        \"\"\" Save Vocab to file as JSON dump.\n        @param file_path (str): file path to vocab file\n        \"\"\"\n        json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), open(file_path, 'w'), indent=2)\n\n    @staticmethod\n    def load(file_path):\n        \"\"\" Load vocabulary from JSON dump.\n        @param file_path (str): file path to vocab file\n        @returns Vocab object loaded from JSON dump\n        \"\"\"\n        entry = json.load(open(file_path, 'r'))\n        src_word2id = entry['src_word2id']\n        tgt_word2id = entry['tgt_word2id']\n\n        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n\n    def __repr__(self):\n        \"\"\" Representation of Vocab to be used\n        when printing the object.\n        \"\"\"\n        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))","execution_count":null,"outputs":[]},{"metadata":{"id":"2aAF0HzU9-gR","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"class CharDecoder(nn.Module):\n    def __init__(self, hidden_size, char_embedding_size=50, target_vocab=None):\n        \"\"\" Init Character Decoder.\n\n        @param hidden_size (int): Hidden size of the decoder LSTM\n        @param char_embedding_size (int): dimensionality of character embeddings\n        @param target_vocab (VocabEntry): vocabulary for the target language. See vocab.py for documentation.\n        \"\"\"\n        ### YOUR CODE HERE for part 2a\n        ### TODO - Initialize as an nn.Module.\n        ###      - Initialize the following variables:\n        ###        self.charDecoder: LSTM. Please use nn.LSTM() to construct this.\n        ###        self.char_output_projection: Linear layer, called W_{dec} and b_{dec} in the PDF\n        ###        self.decoderCharEmb: Embedding matrix of character embeddings\n        ###        self.target_vocab: vocabulary for the target language\n        ###\n        ### Hint: - Use target_vocab.char2id to access the character vocabulary for the target language.\n        ###       - Set the padding_idx argument of the embedding matrix.\n        ###       - Create a new Embedding layer. Do not reuse embeddings created in Part 1 of this assignment.\n        super(CharDecoder, self).__init__()\n        self.charDecoder = nn.LSTM(char_embedding_size, hidden_size)\n        total_target_char_count = len(target_vocab.char2id)\n        self.char_output_projection = nn.Linear(hidden_size, total_target_char_count)\n        # So we don't use pre-trained embed vectors, like GLOVE in these 2 assignments. Could it be used to improve?\n        self.decoderCharEmb = nn.Embedding(total_target_char_count, char_embedding_size,\n                                           padding_idx=target_vocab.char2id['<pad>'])\n        self.target_vocab = target_vocab\n        ### END YOUR CODE\n    \n    def forward(self, input, dec_hidden=None):\n        \"\"\" Forward pass of character decoder.\n\n        @param input: tensor of integers, shape (length, batch)\n        @param dec_hidden: internal state of the LSTM before reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)\n\n        @returns scores: called s_t in the PDF, shape (length, batch, self.vocab_size)\n        @returns dec_hidden: internal state of the LSTM after reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)\n        \"\"\"\n        ### YOUR CODE HERE for part 2b\n        ### TODO - Implement the forward pass of the character decoder.\n        embeded_input = self.decoderCharEmb(input)                          # shape (length, batch, char_embedding_size)\n        output, dec_hidden = self.charDecoder(embeded_input, dec_hidden)    # output shape (length, batch, hidden_size)\n        scores = self.char_output_projection(output)\n        return scores, dec_hidden\n        ### END YOUR CODE \n\n    def train_forward(self, char_sequence, dec_hidden=None):\n        \"\"\" Forward computation during training.\n\n        @param char_sequence: tensor of integers, shape (length, batch). Note that \"length\" here and in forward() need not be the same.\n        @param dec_hidden: initial internal state of the LSTM, obtained from the output of the word-level decoder. A tuple of two tensors of shape (1, batch, hidden_size)\n\n        @returns The cross-entropy loss, computed as the *sum* of cross-entropy losses of all the words in the batch.\n        \"\"\"\n        ### YOUR CODE HERE for part 2c\n        ### TODO - Implement training forward pass.\n        ###\n        ### Hint: - Make sure padding characters do not contribute to the cross-entropy loss.\n        ###       - char_sequence corresponds to the sequence x_1 ... x_{n+1} from the handout (e.g., <START>,m,u,s,i,c,<END>).\n        input = char_sequence[:-1]\n        targets = char_sequence[1:]       # (length - 1, batch)\n        logits, _ = self.forward(input, dec_hidden)     # (length - 1, batch, char_count)\n        # Use ignore_index to filter the padded chars\n        loss = nn.CrossEntropyLoss(ignore_index=self.target_vocab.char2id['<pad>'], reduction='sum')\n        output = loss(logits.view(-1, len(self.target_vocab.char2id)), targets.contiguous().view(-1))\n        return output\n        ### END YOUR CODE\n\n    def decode_greedy(self, initialStates, device, max_length=21):\n        \"\"\" Greedy decoding\n        @param initialStates: initial internal state of the LSTM, a tuple of two tensors of size (1, batch, hidden_size)\n        @param device: torch.device (indicates whether the model is on CPU or GPU)\n        @param max_length: maximum length of words to decode\n\n        @returns decodedWords: a list (of length batch) of strings, each of which has length <= max_length.\n                              The decoded strings should NOT contain the start-of-word and end-of-word characters.\n        \"\"\"\n\n        ### YOUR CODE HERE for part 2d\n        ### TODO - Implement greedy decoding.\n        ### Hints:\n        ###      - Use target_vocab.char2id and target_vocab.id2char to convert between integers and characters\n        ###      - Use torch.tensor(..., device=device) to turn a list of character indices into a tensor.\n        ###      - We use curly brackets as start-of-word and end-of-word characters. That is, use the character '{' for <START> and '}' for <END>.\n        ###        Their indices are self.target_vocab.start_of_word and self.target_vocab.end_of_word, respectively.\n        _, batch, _ = initialStates[0].size()\n        target_vocab = self.target_vocab\n        dec_hidden = initialStates\n        softmax = nn.Softmax(dim=-1)\n        output_words = [\"\" for _ in range(batch)]\n        input = torch.tensor([target_vocab.char2id['{'] for _ in range(batch)], device=device).unsqueeze(0)    # (1, batch)\n        for t in range(max_length):\n            # Need a tensor whose shape is (length, batch)\n            logits, dec_hidden = self.forward(input, dec_hidden)\n            scores = softmax(logits)        # (1, batch, char_count)\n            input = torch.argmax(scores, dim=-1)\n            output_chars = input.squeeze(0).tolist()\n            output_words = [(output_words[i] + target_vocab.id2char[output_chars[i]]) for i in range(batch)]\n        decoded_words = [output_word[:output_word.find(\"}\")] if \"}\" in output_word else output_word for output_word in output_words]\n        return decoded_words\n        ### END YOUR CODE","execution_count":null,"outputs":[]},{"metadata":{"id":"61afVLES9-gT","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self, char_embed_size, word_embed_size, max_word_length=21, kernel_size=5):\n        # Use a kernel size k=5\n        super(CNN, self).__init__()\n        self.conv1d = nn.Conv1d(char_embed_size, word_embed_size, kernel_size)\n        self.max_pool = nn.MaxPool1d(max_word_length - kernel_size + 1)\n\n    def forward(self, x_reshaped):\n        \"\"\"\n        :param x_reshaped: The tensor of char embedding before into the conv network. Size: [batch_size, char_embed_size, max_word_length]\n        :return: x_conv_out: The tensor out of conv layer. Size: [batch_size, word_embed_size]\n        \"\"\"\n        x_conv = self.conv1d(x_reshaped)        # [batch_size, word_embed_size, (max_word_length - kernel_size + 1)]\n        x_conv_out = self.max_pool(x_conv).squeeze(-1)      # [batch_size, word_embed_size]\n        return x_conv_out\n\n### END YOUR CODE","execution_count":null,"outputs":[]},{"metadata":{"id":"wRqs7E329-gV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"23b3dfce-4f27-4c18-efcd-0cf419ab2578","trusted":true},"cell_type":"code","source":"class Highway(nn.Module):\n    def __init__(self, word_embed_size):\n        \"\"\"\n        :param word_embed_size: The embed size of word\n        \"\"\"\n        super(Highway, self).__init__()\n        self.proj = nn.Linear(word_embed_size, word_embed_size)\n        self.gate = nn.Linear(word_embed_size, word_embed_size)\n\n    def forward(self, x_conv_out):\n        \"\"\"\n        :param x_conv_out: The tensor out of conv layer [batch_size, word_embed_size]\n        :return x_highway: The tensor out of highway network [batch_size, word_embed_size]\n        \"\"\"\n        # Operate on batches of words\n        # Be sure to use 2 nn.Linear layers\n        x_proj = F.relu(self.proj(x_conv_out))\n        x_gate = torch.sigmoid(self.proj(x_conv_out))\n        x_highway = x_gate * x_proj + (1 - x_gate) * x_conv_out\n        return x_highway\n\n\n### END YOUR CODE\n\n'''\n### No sanity test, you will now write your own code to thoroughly test your implementation\n• Write code to check that the input and output have the expected shapes and types. Before you\ndo this, make sure you've written docstrings for init () and forward() { you can't test\nthe expected output if you haven't clearly laid out what you expect it to be!\n• Print out the shape of every intermediate value; verify all the shapes are correct.\n• Create a small instance of your highway network (with small, manageable dimensions), manually\ndefine some input, manually de\fne the weights and biases, manually calculate what the output\nshould be, and verify that your module does indeed return that value.\n• Similar to previous, but you could instead print all intermediate values and check each is correct.\n• If you can think of any `edge case' or `unusual' inputs, create test cases based on those.\n'''","execution_count":null,"outputs":[]},{"metadata":{"id":"5G9EGZ4x9-gX","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"class ModelEmbeddings(nn.Module): \n    \"\"\"\n    Class that converts input words to their CNN-based embeddings.\n    \"\"\"\n    def __init__(self, embed_size, vocab):\n        \"\"\"\n        Init the Embedding layer for one language\n        @param embed_size (int): Embedding size (dimensionality) for the output \n        @param vocab (VocabEntry): VocabEntry object. See vocab.py for documentation.\n        \"\"\"\n        super(ModelEmbeddings, self).__init__()\n\n        ## A4 code\n        # pad_token_idx = vocab.src['<pad>']\n        # self.embeddings = nn.Embedding(len(vocab.src), embed_size, padding_idx=pad_token_idx)\n        ## End A4 code\n\n        ### YOUR CODE HERE for part 1j\n        dropout = 0.3\n        char_embed_size = 50\n        pad_char_idx = vocab.char2id['<pad>']\n        self.embed_size = embed_size\n        self.embeddings = nn.Embedding(len(vocab.char2id), char_embed_size, padding_idx=pad_char_idx)\n        self.conv1d = CNN(char_embed_size, embed_size)\n        self.highway = Highway(embed_size)\n        self.dropout = nn.Dropout(dropout)\n        ### END YOUR CODE\n\n    def forward(self, input):\n        \"\"\"\n        Looks up character-based CNN embeddings for the words in a batch of sentences.\n        @param input: Tensor of integers of shape (sentence_length, batch_size, max_word_length) where\n            each integer is an index into the character vocabulary\n\n        @param output: Tensor of shape (sentence_length, batch_size, embed_size), containing the \n            CNN-based embeddings for each word of the sentences in the batch\n        \"\"\"\n        ## A4 code\n        # output = self.embeddings(input)\n        # return output\n        ## End A4 code\n\n        ### YOUR CODE HERE for part 1j\n        char_embeddings = self.embeddings(input)    # (sentence_length, batch_size, max_word_length, char_embed_size)\n        sentence_length, batch_size, max_word_length, char_embed_size = char_embeddings.size()\n        # Need to cast 4D tensor to 3D as only 3D could be the input of conv1d layer. Is there any other way?\n        char_embeddings_reshape = char_embeddings.transpose(2, 3).view(-1, char_embed_size, max_word_length)\n        x_conv_out = self.conv1d(char_embeddings_reshape)\n        x_highway = self.highway(x_conv_out)\n        x_word_emb = self.dropout(x_highway).view(sentence_length, batch_size, self.embed_size)\n        return x_word_emb\n        ### END YOUR CODE","execution_count":null,"outputs":[]},{"metadata":{"id":"m8opAVle9-gZ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nHypothesis = namedtuple('Hypothesis', ['value', 'score'])\n\nclass NMT(nn.Module):\n    \"\"\" Simple Neural Machine Translation Model:\n        - Bidrectional LSTM Encoder\n        - Unidirection LSTM Decoder\n        - Global Attention Model (Luong, et al. 2015)\n    \"\"\"\n    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2, no_char_decoder=False):\n        \"\"\" Init NMT Model.\n\n        @param embed_size (int): Embedding size (dimensionality)\n        @param hidden_size (int): Hidden Size (dimensionality)\n        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n                              See vocab.py for documentation.\n        @param dropout_rate (float): Dropout probability, for attention\n        \"\"\"\n        super(NMT, self).__init__()\n\n        self.model_embeddings_source = ModelEmbeddings(embed_size, vocab.src)\n        self.model_embeddings_target = ModelEmbeddings(embed_size, vocab.tgt)\n\n        self.hidden_size = hidden_size\n        self.dropout_rate = dropout_rate\n        self.vocab = vocab\n\n        ### COPY OVER YOUR CODE FROM ASSIGNMENT 4\n        self.encoder = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n        self.decoder = nn.LSTMCell(embed_size + hidden_size, hidden_size)\n        # Why no bias?\n        self.h_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n        self.c_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n        self.att_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n        self.combined_output_projection = nn.Linear(3 * hidden_size, hidden_size, bias=False)\n        self.target_vocab_projection = nn.Linear(hidden_size, len(vocab.tgt))\n        self.dropout = nn.Dropout(dropout_rate)\n\n        ### END YOUR CODE FROM ASSIGNMENT 4\n\n        if not no_char_decoder:\n           self.charDecoder = CharDecoder(hidden_size, target_vocab=vocab.tgt)\n        else:\n           self.charDecoder = None\n\n    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n        \"\"\" Take a mini-batch of source and target sentences, compute the log-likelihood of\n        target sentences under the language models learned by the NMT system.\n\n        @param source (List[List[str]]): list of source sentence tokens\n        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`\n\n        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n                                    log-likelihood of generating the gold-standard target sentence for\n                                    each example in the input batch. Here b = batch size.\n        \"\"\"\n        # Compute sentence lengths\n        source_lengths = [len(s) for s in source]\n\n        # Convert list of lists into tensors\n\n        ## A4 code\n        # source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   # Tensor: (src_len, b)\n        # target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, b)\n\n        # enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n        # enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n        # combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n        ## End A4 code\n\n        ### YOUR CODE HERE for part 1k\n        ### TODO:\n        ###     Modify the code lines above as needed to fetch the character-level tensor\n        ###     to feed into encode() and decode(). You should:\n        ###     - Keep `target_padded` from A4 code above for predictions\n        ###     - Add `source_padded_chars` for character level padded encodings for source\n        ###     - Add `target_padded_chars` for character level padded encodings for target\n        ###     - Modify calls to encode() and decode() to use the character level encodings\n        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)  # Tensor: (tgt_len, b)\n        source_padded_chars = self.vocab.src.to_input_tensor_char(source, device=self.device)\n        target_padded_chars = self.vocab.tgt.to_input_tensor_char(target, device=self.device)\n\n        enc_hiddens, dec_init_state = self.encode(source_padded_chars, source_lengths)\n        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded_chars)\n        ### END YOUR CODE\n\n        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)\n\n        # Zero out, probabilities for which we have nothing in the target text\n        target_masks = (target_padded != self.vocab.tgt['<pad>']).float()\n\n        # Compute log probability of generating true target words\n        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]\n        scores = target_gold_words_log_prob.sum() # mhahn2 Small modification from A4 code.\n\n\n\n        if self.charDecoder is not None:\n            max_word_len = target_padded_chars.shape[-1]\n\n            target_words = target_padded[1:].contiguous().view(-1)\n            target_chars = target_padded_chars[1:].view(-1, max_word_len)\n            target_outputs = combined_outputs.view(-1, 256)\n\n            target_chars_oov = target_chars #torch.index_select(target_chars, dim=0, index=oovIndices)\n            rnn_states_oov = target_outputs #torch.index_select(target_outputs, dim=0, index=oovIndices)\n            oovs_losses = self.charDecoder.train_forward(target_chars_oov.t(), (rnn_states_oov.unsqueeze(0), rnn_states_oov.unsqueeze(0)))\n            scores = scores - oovs_losses\n\n        return scores\n\n\n    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b, max_word_length), where\n                                        b = batch_size, src_len = maximum source sentence length. Note that\n                                       these have already been sorted in order of longest to shortest sentence.\n        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n                                                hidden state and cell.\n        \"\"\"\n        enc_hiddens, dec_init_state = None, None\n\n        ### COPY OVER YOUR CODE FROM ASSIGNMENT 4\n        ### Except replace \"self.model_embeddings.source\" with \"self.model_embeddings_source\"\n        X = self.model_embeddings_source(source_padded)  # (src_len, b, e)\n        packed_sequence = pack_padded_sequence(X, source_lengths)\n        output, (last_hidden, last_cell) = self.encoder(packed_sequence)\n        enc_hiddens, enc_length = pad_packed_sequence(output)\n        enc_hiddens.transpose_(0, 1)\n        layer_num, batch_size, _ = last_hidden.size()\n        last_hidden = last_hidden.permute(1, 0, 2)\n        last_hidden = last_hidden.contiguous().view(batch_size, -1)\n        last_cell = torch.cat([last_cell[i] for i in range(layer_num)], 1)\n        init_decoder_hidden = self.h_projection(last_hidden)\n        init_decoder_cell = self.c_projection(last_cell)\n        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n        ### END YOUR CODE\n\n        ### END YOUR CODE FROM ASSIGNMENT 4\n\n        return enc_hiddens, dec_init_state\n\n\n    def decode(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,\n                dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor) -> torch.Tensor:\n        \"\"\"Compute combined output vectors for a batch.\n        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where\n                                     b = batch size, src_len = maximum source sentence length.\n        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b, max_word_length), where\n                                       tgt_len = maximum target sentence length, b = batch size.\n        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where\n                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n        \"\"\"\n        # Chop of the <END> token for max length sentences.\n        target_padded = target_padded[:-1]\n\n        # Initialize the decoder state (hidden and cell)\n        dec_state = dec_init_state\n\n        # Initialize previous combined output vector o_{t-1} as zero\n        batch_size = enc_hiddens.size(0)\n        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n\n        # Initialize a list we will use to collect the combined output o_t on each step\n        combined_outputs = []\n\n        ### COPY OVER YOUR CODE FROM ASSIGNMENT 4\n        ### Except replace \"self.model_embeddings.target\" with \"self.model_embeddings_target\"\n        enc_hiddens_proj = self.att_projection(enc_hiddens)  # (b, src_len, h)     torch.matmul( ,\n        Y = self.model_embeddings_target(target_padded)  # (tgt_len, b, e)\n        Y_splited = torch.split(Y, 1)\n        output_list = []\n        for Y_t in Y_splited:\n            Y_t = torch.squeeze(Y_t, 0)  # (b, e)\n            Ybar_t = torch.cat((Y_t, o_prev), dim=1)  # (b, h + e)\n            dec_state, o_t, e_t = self.step(Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks)\n            output_list.append(o_t)\n            o_prev = o_t\n        combined_outputs = torch.stack(output_list)\n        ### END YOUR CODE FROM ASSIGNMENT 4\n\n        return combined_outputs\n\n\n    def step(self, Ybar_t: torch.Tensor,\n            dec_state: Tuple[torch.Tensor, torch.Tensor],\n            enc_hiddens: torch.Tensor,\n            enc_hiddens_proj: torch.Tensor,\n            enc_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n        \"\"\" Compute one forward step of the LSTM decoder, including the attention computation.\n        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n                                where b = batch size, e = embedding size, h = hidden size.\n        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n                First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.\n        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n                                    src_len = maximum source length, h = hidden size.\n        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n                                    where b = batch size, src_len = maximum source length, h = hidden size.\n        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),\n                                    where b = batch size, src_len is maximum source length.\n        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n                First tensor is decoder's new hidden state, second tensor is decoder's new cell.\n        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n                                Note: You will not use this outside of this function.\n                                      We are simply returning this value so that we can sanity check\n                                      your implementation.\n        \"\"\"\n\n        combined_output = None\n\n        ### COPY OVER YOUR CODE FROM ASSIGNMENT 4\n        dec_state = self.decoder(Ybar_t, dec_state)\n        dec_hidden, dec_cell = dec_state  # (b, h)\n        e_t = torch.bmm(enc_hiddens_proj, dec_hidden.unsqueeze(dim=2))  # (b, src_len, h) @ (b, h, 1) -> (b, src_len, 1)\n        e_t.squeeze_(dim=2)\n        ### END YOUR CODE FROM ASSIGNMENT 4\n\n        # Set e_t to -inf where enc_masks has 1\n        if enc_masks is not None:\n            e_t.data.masked_fill_(enc_masks.bool(), -float('inf'))\n\n        ### COPY OVER YOUR CODE FROM ASSIGNMENT 4\n        alpha_t = F.softmax(e_t, dim=1)  # (b, src_len)\n        a_t = torch.bmm(alpha_t.unsqueeze(dim=1), enc_hiddens)  # (b, 1, 2h)\n        a_t.squeeze_(dim=1)  # (b, h)\n        U_t = torch.cat((dec_hidden, a_t), dim=1)  # (b, 3h)\n        V_t = self.combined_output_projection(U_t)  # (b, h)\n        O_t = self.dropout(torch.tanh(V_t))\n        ### END YOUR CODE FROM ASSIGNMENT 4\n\n        combined_output = O_t\n        return dec_state, combined_output, e_t\n\n    def generate_sent_masks(self, enc_hiddens: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n        \"\"\" Generate sentence masks for encoder hidden states.\n\n        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n                                     src_len = max source length, h = hidden size.\n        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n\n        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n                                    where src_len = max source length, h = hidden size.\n        \"\"\"\n        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n        for e_id, src_len in enumerate(source_lengths):\n            enc_masks[e_id, src_len:] = 1\n        return enc_masks.to(self.device)\n\n\n    def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n        @param src_sent (List[str]): a single source sentence (words)\n        @param beam_size (int): beam size\n        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n                value: List[str]: the decoded target sentence, represented as a list of words\n                score: float: the log-likelihood of the target sentence\n        \"\"\"\n        ## A4 code\n        # src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n        ## End A4 code\n\n        src_sents_var = self.vocab.src.to_input_tensor_char([src_sent], self.device)\n\n        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n        src_encodings_att_linear = self.att_projection(src_encodings)\n\n        h_tm1 = dec_init_vec\n        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n\n        eos_id = self.vocab.tgt['</s>']\n\n        hypotheses = [['<s>']]\n        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n        completed_hypotheses = []\n\n\n        t = 0\n        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n            t += 1\n            hyp_num = len(hypotheses)\n\n            exp_src_encodings = src_encodings.expand(hyp_num,\n                                                     src_encodings.size(1),\n                                                     src_encodings.size(2))\n\n            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n                                                                           src_encodings_att_linear.size(1),\n                                                                           src_encodings_att_linear.size(2))\n\n            ## A4 code\n            # y_tm1 = self.vocab.tgt.to_input_tensor(list([hyp[-1]] for hyp in hypotheses), device=self.device)\n            # y_t_embed = self.model_embeddings_target(y_tm1)\n            ## End A4 code\n\n            y_tm1 = self.vocab.tgt.to_input_tensor_char(list([hyp[-1]] for hyp in hypotheses), device=self.device)\n            y_t_embed = self.model_embeddings_target(y_tm1)\n            y_t_embed = torch.squeeze(y_t_embed, dim=0)\n\n\n            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n\n            (h_t, cell_t), att_t, _  = self.step(x, h_tm1,\n                                                      exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n\n            # log probabilities over target words\n            log_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n\n            live_hyp_num = beam_size - len(completed_hypotheses)\n            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n\n            prev_hyp_ids = top_cand_hyp_pos / len(self.vocab.tgt)\n            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n\n            new_hypotheses = []\n            live_hyp_ids = []\n            new_hyp_scores = []\n\n            decoderStatesForUNKsHere = []\n            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n                prev_hyp_id = prev_hyp_id.item()\n                hyp_word_id = hyp_word_id.item()\n                cand_new_hyp_score = cand_new_hyp_score.item()\n\n                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n\n                # Record output layer in case UNK was generated\n                if hyp_word == \"<unk>\":\n                   hyp_word = \"<unk>\"+str(len(decoderStatesForUNKsHere))\n                   decoderStatesForUNKsHere.append(att_t[prev_hyp_id])\n\n                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n                if hyp_word == '</s>':\n                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n                                                           score=cand_new_hyp_score))\n                else:\n                    new_hypotheses.append(new_hyp_sent)\n                    live_hyp_ids.append(prev_hyp_id)\n                    new_hyp_scores.append(cand_new_hyp_score)\n\n            if len(decoderStatesForUNKsHere) > 0 and self.charDecoder is not None: # decode UNKs\n                decoderStatesForUNKsHere = torch.stack(decoderStatesForUNKsHere, dim=0)\n                decodedWords = self.charDecoder.decode_greedy((decoderStatesForUNKsHere.unsqueeze(0), decoderStatesForUNKsHere.unsqueeze(0)), max_length=21, device=self.device)\n                assert len(decodedWords) == decoderStatesForUNKsHere.size()[0], \"Incorrect number of decoded words\"\n                for hyp in new_hypotheses:\n                  if hyp[-1].startswith(\"<unk>\"):\n                        hyp[-1] = decodedWords[int(hyp[-1][5:])]#[:-1]\n\n            if len(completed_hypotheses) == beam_size:\n                break\n\n            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n            att_tm1 = att_t[live_hyp_ids]\n\n            hypotheses = new_hypotheses\n            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n\n        if len(completed_hypotheses) == 0:\n            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n                                                   score=hyp_scores[0].item()))\n\n        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n        return completed_hypotheses\n\n    @property\n    def device(self) -> torch.device:\n        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n        \"\"\"\n        return self.att_projection.weight.device\n\n    @staticmethod\n    def load(model_path: str, no_char_decoder=False):\n        \"\"\" Load the model from a file.\n        @param model_path (str): path to model\n        \"\"\"\n        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n        args = params['args']\n        model = NMT(vocab=params['vocab'], no_char_decoder=no_char_decoder, **args)\n        model.load_state_dict(params['state_dict'])\n\n        return model\n\n    def save(self, path: str):\n        \"\"\" Save the odel to a file.\n        @param path (str): path to the model\n        \"\"\"\n        print('save model parameters to [%s]' % path, file=sys.stderr)\n\n        params = {\n            'args': dict(embed_size=self.model_embeddings_source.embed_size, hidden_size=self.hidden_size, dropout_rate=self.dropout_rate),\n            'vocab': self.vocab,\n            'state_dict': self.state_dict()\n        }\n\n        torch.save(params, path)","execution_count":null,"outputs":[]},{"metadata":{"id":"OZw4lKMP9-gb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"68ef59e8-676c-44fa-ee16-baee3777d212","trusted":true},"cell_type":"code","source":"def evaluate_ppl(model, dev_data, batch_size=32):\n    \"\"\" Evaluate perplexity on dev sentences\n    @param model (NMT): NMT Model\n    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n    @param batch_size (batch size)\n    @returns ppl (perplixty on dev sentences)\n    \"\"\"\n    was_training = model.training\n    model.eval()\n\n    cum_loss = 0.\n    cum_tgt_words = 0.\n\n    # no_grad() signals backend to throw away all gradients\n    with torch.no_grad():\n        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n            loss = -model(src_sents, tgt_sents).sum()\n\n            cum_loss += loss.item()\n            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n            cum_tgt_words += tgt_word_num_to_predict\n\n        ppl = np.exp(cum_loss / cum_tgt_words)\n\n    if was_training:\n        model.train()\n\n    return ppl\n\n\ndef compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n    @param references (List[List[str]]): a list of gold-standard reference target sentences\n    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n    @returns bleu_score: corpus-level BLEU score\n    \"\"\"\n    if references[0][0] == '<s>':\n        references = [ref[1:-1] for ref in references]\n    bleu_score = corpus_bleu([[ref] for ref in references],\n                             [hyp.value for hyp in hypotheses])\n    return bleu_score\n\n\ndef train(args: Dict):\n    \"\"\" Train the NMT Model.\n    @param args (Dict): args from cmd line\n    \"\"\"\n    train_data_src = read_corpus(args['--train-src'], source='src')\n    train_data_tgt = read_corpus(args['--train-tgt'], source='tgt')\n\n    dev_data_src = read_corpus(args['--dev-src'], source='src')\n    dev_data_tgt = read_corpus(args['--dev-tgt'], source='tgt')\n\n    train_data = list(zip(train_data_src, train_data_tgt))\n    dev_data = list(zip(dev_data_src, dev_data_tgt))\n\n    train_batch_size = int(args['--batch-size'])\n\n    clip_grad = float(args['--clip-grad'])\n    valid_niter = int(args['--valid-niter'])\n    log_every = int(args['--log-every'])\n    model_save_path = args['--save-to']\n\n    vocab = Vocab.load(args['--vocab'])\n\n    model = NMT(embed_size=int(args['--embed-size']),\n                hidden_size=int(args['--hidden-size']),\n                dropout_rate=float(args['--dropout']),\n                vocab=vocab, no_char_decoder=args['--no-char-decoder'])\n    model.train()\n\n    uniform_init = float(args['--uniform-init'])\n    if np.abs(uniform_init) > 0.:\n        print('uniformly initialize parameters [-%f, +%f]' % (uniform_init, uniform_init), file=sys.stderr)\n        for p in model.parameters():\n            p.data.uniform_(-uniform_init, uniform_init)\n\n    vocab_mask = torch.ones(len(vocab.tgt))\n    vocab_mask[vocab.tgt['<pad>']] = 0\n\n    device = torch.device(\"cuda:0\" if args['--cuda'] else \"cpu\")\n    print('use device: %s' % device, file=sys.stderr)\n\n    model = model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=float(args['--lr']))\n\n    num_trial = 0\n    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n    cum_examples = report_examples = epoch = valid_num = 0\n    hist_valid_scores = []\n    train_time = begin_time = time.time()\n    print('begin Maximum Likelihood training')\n\n    while True:\n        epoch += 1\n\n        for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n            train_iter += 1\n\n            optimizer.zero_grad()\n\n            batch_size = len(src_sents)\n\n            example_losses = -model(src_sents, tgt_sents) # (batch_size,)\n            batch_loss = example_losses.sum()\n            loss = batch_loss / batch_size\n\n            loss.backward()\n\n            # clip gradient\n            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n\n            optimizer.step()\n\n            batch_losses_val = batch_loss.item()\n            report_loss += batch_losses_val\n            cum_loss += batch_losses_val\n\n            tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n            report_tgt_words += tgt_words_num_to_predict\n            cum_tgt_words += tgt_words_num_to_predict\n            report_examples += batch_size\n            cum_examples += batch_size\n\n            if train_iter % log_every == 0:\n                print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n                      'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,\n                                                                                         report_loss / report_examples,\n                                                                                         math.exp(report_loss / report_tgt_words),\n                                                                                         cum_examples,\n                                                                                         report_tgt_words / (time.time() - train_time),\n                                                                                         time.time() - begin_time), file=sys.stderr)\n\n                train_time = time.time()\n                report_loss = report_tgt_words = report_examples = 0.\n\n            # perform validation\n            if train_iter % valid_niter == 0:\n                print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,\n                                                                                         cum_loss / cum_examples,\n                                                                                         np.exp(cum_loss / cum_tgt_words),\n                                                                                         cum_examples), file=sys.stderr)\n\n                cum_loss = cum_examples = cum_tgt_words = 0.\n                valid_num += 1\n\n                print('begin validation ...', file=sys.stderr)\n\n                # compute dev. ppl and bleu\n                dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n                valid_metric = -dev_ppl\n\n                print('validation: iter %d, dev. ppl %f' % (train_iter, dev_ppl), file=sys.stderr)\n\n                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n                hist_valid_scores.append(valid_metric)\n\n                if is_better:\n                    patience = 0\n                    print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n                    model.save(model_save_path)\n\n                    # also save the optimizers' state\n                    torch.save(optimizer.state_dict(), model_save_path + '.optim')\n                elif patience < int(args['--patience']):\n                    patience += 1\n                    print('hit patience %d' % patience, file=sys.stderr)\n\n                    if patience == int(args['--patience']):\n                        num_trial += 1\n                        print('hit #%d trial' % num_trial, file=sys.stderr)\n                        if num_trial == int(args['--max-num-trial']):\n                            print('early stop!', file=sys.stderr)\n                            exit(0)\n\n                        # decay lr, and restore from previously best checkpoint\n                        lr = optimizer.param_groups[0]['lr'] * float(args['--lr-decay'])\n                        print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n\n                        # load model\n                        params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n                        model.load_state_dict(params['state_dict'])\n                        model = model.to(device)\n\n                        print('restore parameters of the optimizers', file=sys.stderr)\n                        optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n\n                        # set new lr\n                        for param_group in optimizer.param_groups:\n                            param_group['lr'] = lr\n\n                        # reset patience\n                        patience = 0\n\n            if epoch == int(args['--max-epoch']):\n                print('reached maximum number of epochs!', file=sys.stderr)\n                # exit(0)\n                raise KeyboardInterrupt\n\n\ndef decode(args: Dict[str, str]):\n    \"\"\" Performs decoding on a test set, and save the best-scoring decoding results.\n    If the target gold-standard sentences are given, the function also computes\n    corpus-level BLEU score.\n    @param args (Dict): args from cmd line\n    \"\"\"\n\n    print(\"load test source sentences from [{}]\".format(args['TEST_SOURCE_FILE']), file=sys.stderr)\n    test_data_src = read_corpus(args['TEST_SOURCE_FILE'], source='src')\n    if args['TEST_TARGET_FILE']:\n        print(\"load test target sentences from [{}]\".format(args['TEST_TARGET_FILE']), file=sys.stderr)\n        test_data_tgt = read_corpus(args['TEST_TARGET_FILE'], source='tgt')\n\n    print(\"load model from {}\".format(args['MODEL_PATH']), file=sys.stderr)\n    model = NMT.load(args['MODEL_PATH'], no_char_decoder=args['--no-char-decoder'])\n\n    if args['--cuda']:\n        model = model.to(torch.device(\"cuda:0\"))\n\n    hypotheses = beam_search(model, test_data_src,\n                             beam_size=int(args['--beam-size']),\n                             max_decoding_time_step=int(args['--max-decoding-time-step']))\n\n    if args['TEST_TARGET_FILE']:\n        top_hypotheses = [hyps[0] for hyps in hypotheses]\n        bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n        print('Corpus BLEU: {}'.format(bleu_score * 100), file=sys.stderr)\n\n    with open(args['OUTPUT_FILE'], 'w') as f:\n        for src_sent, hyps in zip(test_data_src, hypotheses):\n            top_hyp = hyps[0]\n            hyp_sent = ' '.join(top_hyp.value)\n            f.write(hyp_sent + '\\n')\n\n\ndef beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n    \"\"\" Run beam search to construct hypotheses for a list of src-language sentences.\n    @param model (NMT): NMT Model\n    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n    \"\"\"\n    was_training = model.training\n    model.eval()\n\n    hypotheses = []\n    with torch.no_grad():\n        for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):\n            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n\n            hypotheses.append(example_hyps)\n\n    if was_training: model.train(was_training)\n\n    return hypotheses\n\n\ndef main():\n    \"\"\" Main func.\n    \"\"\"\n    # args = docopt(__doc__)\n\n    # Check pytorch version\n    assert(torch.__version__ >= \"1.0.0\"), \"Please update your installation of PyTorch. You have {} and you should have version 1.0.0\".format(torch.__version__)\n\n    # seed the random number generators\n    seed = int(args['--seed'])\n    torch.manual_seed(seed)\n    if args['--cuda']:\n        torch.cuda.manual_seed(seed)\n    np.random.seed(seed * 13 // 7)\n\n    if args['train']:\n        train(args)\n    elif args['decode']:\n        decode(args)\n    else:\n        raise RuntimeError('invalid run mode')\n\n\nif __name__ == '__main__':\n    main()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.0"},"colab":{"name":"assignment5_google_colab.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}